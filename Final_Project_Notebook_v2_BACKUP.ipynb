{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project Analysis of Winter Food & Beverages Market For STAT112 Lecture\n",
                "\n",
                "## Group Members\n",
                "- **Barış**\n",
                "- **Ali**\n",
                "- **Batuhan**\n",
                "- **Oğuz Kerem Ayhan**\n",
                "\n",
                "## Dataset Summary\n",
                "This project analyzes the `winter_food_beverages_synthetic_1000_dirty.csv` dataset, which contains synthetic sales data for food and beverage products. The dataset includes information on sales channels, unit prices, discount rates, and product details. The goal is to clean the data and perform exploratory data analysis (EDA) to understand market dynamics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd                                                              # Loading a specific tool for our work\n",
                "import numpy as np                                                               # Loading a specific tool for our work\n",
                "import seaborn as sns                                                            # Loading a specific tool for our work\n",
                "import matplotlib.pyplot as plt                                                  # Loading a specific tool for our work\n",
                "import warnings                                                                  # Loading a specific tool for our work\n",
                "warnings.filterwarnings('ignore')                                                # Hiding system warnings for a cleaner output\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Data Pre-processing\n",
                "In this section, we apply data cleaning and tidying techniques following the project checklist requirements step-by-step."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Examine the variables and their data types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset initially to examine it\n",
                "df = pd.read_csv('Group_18_Winter_Food/winter_food_beverages_synthetic_1000_dirty.csv') # Reading the data from a CSV file\n",
                "\n",
                "# Display information about columns and data types\n",
                "print(\"Dataset Info:\")                                                           # Printing information to the screen\n",
                "df.info()                                                                        # Executing this step in data analysis\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Examine the head and tail of the data frame.\n",
                "Make sure that you import your data correctly. Check for any separation argument problem (“;” or “,”) of the data, the existence of header in the dataset as well as the existence of NAs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display the first 5 rows to check import and headers\n",
                "print(\"First 5 rows:\")                                                           # Printing information to the screen\n",
                "display(df.head())                                                               # Executing this step in data analysis\n",
                "\n",
                "# Display the last 5 rows to check for footer issues\n",
                "print(\"\\nLast 5 rows:\")                                                          # Printing information to the screen\n",
                "display(df.tail())                                                               # Executing this step in data analysis\n",
                "\n",
                "# Check for initial missing values (NAs)\n",
                "print(\"\\nInitial Missing Values Check:\")                                         # Printing information to the screen\n",
                "print(df.isnull().sum())                                                         # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Check whether\n",
                "1. Column headers are values, not variable names.\n",
                "2. Multiple variables are stored in one column.\n",
                "3. Variables are stored in both rows and columns.\n",
                "4. Multiple types of observational units are stored in the same table.\n",
                "5. A single observational unit is stored in multiple tables.\n",
                "\n",
                "If so, apply data tidying techniques such as stack/unstack, melt, and pivot. Examine the head and tail of the tidy data frame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Tidying Check\n",
                "# Observation: The dataset appears to be in a tidy format where each row represents a single observation.\n",
                "# No melting or pivoting is required as headers are variables, not values.\n",
                "print(\"Data Tidying Check: Data appears to be tidy. Rows = Observations, Columns = Variables.\") # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Fix the column names if you detect any typos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic cleaning: Strip whitespace from ends and convert all column names to lowercase\n",
                "df.columns = df.columns.str.strip().str.lower()                                  # Cleaning and updating the column names\n",
                "\n",
                "# Remove parentheses and other strictly non-alphanumeric chars (preserving underscores)\n",
                "df.columns = df.columns.str.replace(r'[^\\w\\s]', '', regex=True)                  # Cleaning and updating the column names\n",
                "# Replace spaces with underscores to ensure snake_case format\n",
                "df.columns = df.columns.str.replace(' ', '_')                                    # Cleaning and updating the column names\n",
                "# Replace hyphens with underscores for consistency\n",
                "df.columns = df.columns.str.replace('-', '_')                                    # Cleaning and updating the column names\n",
                "\n",
                "# Explicitly map known messy columns to standard snake_case names expected by analysis\n",
                "rename_map = {                                                                   # Defining a list of names to be corrected\n",
                "    'demandsegment': 'demand_segment',                                           # Executing this step in data analysis\n",
                "    'saleschannel': 'sales_channel',                                             # Executing this step in data analysis\n",
                "    'unitprice_usd': 'unit_price_usd',                                           # Executing this step in data analysis\n",
                "    'discountrate': 'discount_rate',                                             # Executing this step in data analysis\n",
                "    'packagetype': 'package_type',                                               # Executing this step in data analysis\n",
                "    'calorieskcal': 'calories_kcal',                                             # Executing this step in data analysis\n",
                "    'stock_level': 'stock_level'                                                 # Executing this step in data analysis\n",
                "}                                                                                # Executing this step in data analysis\n",
                "# Rename the columns using the dictionary map\n",
                "df = df.rename(columns=rename_map)                                               # Renaming the columns based on our list\n",
                "\n",
                "# Fix any double underscores created by accident during replacement\n",
                "df.columns = df.columns.str.replace('__', '_')                                   # Cleaning and updating the column names\n",
                "\n",
                "# Print the cleaned column names to verify the changes\n",
                "print(\"Cleaned Column Names:\", df.columns.tolist())                              # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Drop unnecessary columns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# In this dataset, all columns appear relevant for analysis initially.\n",
                "# Columns with excessive missing data will be dropped in Step 16 as per the checklist order.\n",
                "# Here we would drop IDs or irrelevant metadata if present.\n",
                "print(\"No specific unnecessary columns identified for immediate removal.\")       # Printing information to the screen\n",
                "# (Placeholder for manual column drops if needed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Remove the duplicates if it is not the nature of the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the number of duplicate rows in the dataset\n",
                "duplicates_count = df.duplicated().sum()                                         # Counting how many rows are identical\n",
                "print(f\"Number of duplicate rows: {duplicates_count}\")                           # Printing information to the screen\n",
                "\n",
                "# Remove duplicate rows if any are found\n",
                "if duplicates_count > 0:                                                         # Executing this step in data analysis\n",
                "    # Drop duplicates in place to keep only unique rows\n",
                "    df = df.drop_duplicates()                                                    # Removing the identical rows from data\n",
                "    print(\"Duplicates removed.\")                                                 # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Get rid of any unnecessary strings in the values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify object (string) columns in the dataframe\n",
                "str_cols = df.select_dtypes(include=['object']).columns                          # Finding all columns that contain text\n",
                "\n",
                "# Special Handling: 'discount_rate' might be read as object due to '%' sign\n",
                "if 'discount_rate' in df.columns and df['discount_rate'].dtype == 'object':      # Executing this step in data analysis\n",
                "    # Remove '%' string from values\n",
                "    df['discount_rate'] = df['discount_rate'].astype(str).str.replace('%', '')   # Removing unwanted characters from the text\n",
                "    print(\"Removed '%' from discount_rate.\")                                     # Printing information to the screen\n",
                "\n",
                "# General cleanup for other string columns: Remove special characters\n",
                "for col in str_cols:                                                             # Executing this step in data analysis\n",
                "    if col == 'discount_rate': continue                                          # Executing this step in data analysis\n",
                "    # Remove strictly non-alphanumeric characters (preserving spaces and hyphens)\n",
                "    df[col] = df[col].astype(str).str.replace(r'[^\\w\\s-]', '', regex=True)       # Removing unwanted characters from the text\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Remove the white spaces in the string values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify object (string) columns\n",
                "str_cols = df.select_dtypes(include=['object']).columns                          # Finding all columns that contain text\n",
                "\n",
                "for col in str_cols:                                                             # Executing this step in data analysis\n",
                "    # Strip whitespace from execution and trailing ends\n",
                "    df[col] = df[col].astype(str).str.strip()                                    # Removing extra spaces from the text\n",
                "    \n",
                "print(\"White spaces removed from string values.\")                                # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Be sure that all strings are in the same format (e.g. all in lower-case). If not, correct them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify object (string) columns\n",
                "str_cols = df.select_dtypes(include=['object']).columns                          # Finding all columns that contain text\n",
                "\n",
                "for col in str_cols:                                                             # Executing this step in data analysis\n",
                "    # Convert all string values to lowercase for consistency\n",
                "    df[col] = df[col].astype(str).str.lower()                                    # Converting all text to small letters\n",
                "    \n",
                "print(\"All string values converted to lower-case.\")                              # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 10. Look at the value counts of strings and be sure that all levels of the categories are unique. If not, correct them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check unique values for categorical columns to spot any typos or duplicates\n",
                "str_cols = df.select_dtypes(include=['object']).columns                          # Finding all columns that contain text\n",
                "\n",
                "print(\"Unique Category Checks:\")                                                 # Printing information to the screen\n",
                "for col in str_cols:                                                             # Executing this step in data analysis\n",
                "    if len(df[col].unique()) < 50:                                               # Executing this step in data analysis\n",
                "        print(f\"\\n{col} unique values:\")                                         # Printing information to the screen\n",
                "        print(df[col].unique())                                                  # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. If you have year, month, and/or day columns, combine them and create a date column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if 'year' and 'season_week' columns exist to create a date\n",
                "if 'year' in df.columns and 'season_week' in df.columns:                         # Executing this step in data analysis\n",
                "    # Ensure 'year' and 'season_week' are numeric\n",
                "    df['year'] = pd.to_numeric(df['year'], errors='coerce')                      # Converting values into actual numbers\n",
                "    df['season_week'] = pd.to_numeric(df['season_week'], errors='coerce')        # Converting values into actual numbers\n",
                "    \n",
                "    # Drop rows with missing temporal data before date creation\n",
                "    df = df.dropna(subset=['year', 'season_week'])                               # Removing rows that have missing values\n",
                "    \n",
                "    # Construct a date string (Year-Week-Day)\n",
                "    # We assume the first day (Monday=1) for the given ISO week\n",
                "    # Executing this step in data analysis\n",
                "    df['date_str'] = df['year'].astype(int).astype(str) + '-W' + \\\n",
                "                     df['season_week'].astype(int).astype(str).str.zfill(2) + '-1' # Executing this step in data analysis\n",
                "    \n",
                "    # Convert constructed string to datetime object\n",
                "    df['date'] = pd.to_datetime(df['date_str'], format='%Y-W%U-%w', errors='coerce') # Converting text into a standardized date format\n",
                "    \n",
                "    # Handle potential ISO format differences if standard conversion failed (NaT)\n",
                "    mask = df['date'].isna()                                                     # Executing this step in data analysis\n",
                "    if mask.any():                                                               # Executing this step in data analysis\n",
                "         df.loc[mask, 'date'] = pd.to_datetime(df.loc[mask, 'date_str'], format='%G-W%V-%u', errors='coerce') # Converting text into a standardized date format\n",
                "            \n",
                "    print(\"Date column created successfully from year and season_week.\")         # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 12. Examine the data types again and be sure that numeric variables are float, categorical ones are object, and date is in date format. If not, correct it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define expected numeric columns\n",
                "numeric_cols = ['unit_price_usd', 'calories_kcal', 'sugar_g', 'spice_index', 'discount_rate', 'units_sold'] # Executing this step in data analysis\n",
                "\n",
                "# Ensure numeric columns are floats\n",
                "for col in numeric_cols:                                                         # Executing this step in data analysis\n",
                "    if col in df.columns:                                                        # Executing this step in data analysis\n",
                "        df[col] = pd.to_numeric(df[col], errors='coerce')                        # Converting values into actual numbers\n",
                "\n",
                "# Verify data types\n",
                "print(\"Final Data Types:\")                                                       # Printing information to the screen\n",
                "df.info()                                                                        # Executing this step in data analysis\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 13. Examine the descriptive statistics of numerical variables. Search for any unusual behavior. Are the variables in the correct range? If not, find the locations and correct them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display descriptive statistics for numerical variables\n",
                "print(\"Descriptive Statistics:\")                                                 # Printing information to the screen\n",
                "display(df.describe())                                                           # Showing the mathematical summary of data\n",
                "\n",
                "# Check ranges (Example: Discount rate should be between 0 and 1 theoretically, though - values handled in outliers)\n",
                "# Here we verify if values look plausible via the description output."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 14. Search for possible outliers. If there are outliers, replace them with the mean."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detect and handle outliers using the 3-Sigma rule (Z-score > 3)\n",
                "numeric_cols_check = df.select_dtypes(include=[float, int]).columns              # Executing this step in data analysis\n",
                "\n",
                "for col in numeric_cols_check:                                                   # Executing this step in data analysis\n",
                "    if col in ['year', 'season_week']:                                           # Executing this step in data analysis\n",
                "        continue                                                                 # Executing this step in data analysis\n",
                "        \n",
                "    mean = df[col].mean()                                                        # Calculating the average value of the column\n",
                "    std = df[col].std()                                                          # Calculating the variation (spread) of data\n",
                "    \n",
                "    # Identify outliers (values more than 3 standard deviations from mean)\n",
                "    outliers = (abs(df[col] - mean) > 3 * std)                                   # Searching for values that are very far from average\n",
                "    \n",
                "    if outliers.sum() > 0:                                                       # Executing this step in data analysis\n",
                "        print(f\"Replacing {outliers.sum()} outliers in {col} with mean {mean:.2f}\") # Printing information to the screen\n",
                "        df.loc[outliers, col] = mean                                             # Replacing the extreme values with the average\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 15. Search for uniformity. The units in the numeric columns are in the same format or not. That is, examine whether some data are in meters but some in centimeters. If they are not consistent, convert them into the same units."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for uniformity\n",
                "# In this dataset, 'unit_price_usd' assumes common currency (USD).\n",
                "# 'calories_kcal' clearly indicates the unit.\n",
                "# No mixed units detected in inspections.\n",
                "print(\"Uniformity Check: Units appear consistent across columns based on header definitions (USD, kcal, grams).\") # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 16. Search for the missing values. Examine their percentage in each column. If the percentage is low, fill them with mean/median/mode. If the percentage is high (e.g >60%-65%), you can drop the column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the percentage of missing values per column\n",
                "missing_percent = df.isnull().mean() * 100                                       # Calculating the percentage of missing values\n",
                "print(\"Missing Value Percentages:\\n\", missing_percent)                           # Printing information to the screen\n",
                "\n",
                "# 1. Drop columns with > 65% missing values\n",
                "cols_to_drop = missing_percent[missing_percent > 65].index                       # Executing this step in data analysis\n",
                "\n",
                "if len(cols_to_drop) > 0:                                                        # Executing this step in data analysis\n",
                "    print(f\"\\nDropping columns (High Missing %): {cols_to_drop.tolist()}\")       # Printing information to the screen\n",
                "    df = df.drop(columns=cols_to_drop)                                           # Removing columns that have too much missing data\n",
                "\n",
                "# 2. Fill remaining missing values\n",
                "for col in df.columns:                                                           # Executing this step in data analysis\n",
                "    if df[col].isnull().sum() > 0:                                               # Executing this step in data analysis\n",
                "        if pd.api.types.is_numeric_dtype(df[col]):                               # Executing this step in data analysis\n",
                "            # Fill numeric with mean\n",
                "            df[col] = df[col].fillna(df[col].mean())                             # Calculating the average value of the column\n",
                "        else:                                                                    # Executing this step in data analysis\n",
                "            # Fill categorical with mode (first mode)\n",
                "            df[col] = df[col].fillna(df[col].mode()[0])                          # Filling in the empty spots with estimated values\n",
                "            \n",
                "print(\"Final check for missing values (should be 0):\")                           # Printing information to the screen\n",
                "print(df.isnull().sum())                                                         # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fully processed data for EDA\n",
                "output_filename = 'cleaned_winter_research_data.csv'                             # Executing this step in data analysis\n",
                "df.to_csv(output_filename, index=False)                                          # Saving our clean data into a new file\n",
                "print(f\"Cleaned data saved to {output_filename}\")                                # Printing information to the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Exploratory Data Analysis (EDA)\n",
                "\n",
                "## Barış - Research Question\n",
                "**How does the Discount Rate impact Units Sold across different Diet Labels?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "df = pd.read_csv('cleaned_winter_research_data.csv')                             # Reading the data from a CSV file\n",
                "plt.figure(figsize=(12, 6))                                                      # Using different visuals to show data groups\n",
                "sns.scatterplot(data=df, x='discount_rate', y='units_sold', hue='diet_label', style='demand_segment', alpha=0.7) # Drawing a scatter plot with dots\n",
                "plt.title('Impact of Discount Rate on Units Sold by Diet Label')                 # Adding a main title at the top\n",
                "plt.xlabel('Discount Rate')                                                      # Labeling the horizontal axis\n",
                "plt.ylabel('Units Sold')                                                         # Labeling the vertical axis\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')                           # Showing the guide box for colors and symbols\n",
                "plt.grid(True)                                                                   # Showing grid lines in the background\n",
                "plt.show()                                                                       # Displaying the final plots on the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ali - Research Question\n",
                "**How does Demand Segment influence the Price-Discount relationship across Sales Channels?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(\"cleaned_winter_research_data.csv\")                             # Reading the data from a CSV file\n",
                "\n",
                "g = sns.FacetGrid(df, col=\"demand_segment\", col_order=['low', 'medium', 'high'], height=5) # Creating a grid structure for multiple plots\n",
                "\n",
                "g.map_dataframe(                                                                 # Drawing a plot on each part of the grid\n",
                "    sns.scatterplot,                                                             # Drawing a scatter plot with dots\n",
                "    x=\"unit_price_usd\",                                                          # Executing this step in data analysis\n",
                "    y=\"discount_rate\",                                                           # Executing this step in data analysis\n",
                "    hue=\"sales_channel\",                                                         # Using different visuals to show data groups\n",
                "    alpha=0.6                                                                    # Setting the transparency of the dots\n",
                ")                                                                                # Executing this step in data analysis\n",
                "\n",
                "g.set_axis_labels(\"Unit Price (USD)\", \"Discount Rate\")                           # Labeling the axes for all small plots\n",
                "g.set_titles(\"Demand: {col_name}\")                                               # Setting titles for individual small plots\n",
                "\n",
                "g.add_legend(                                                                    # Adding a guide box for the whole grid\n",
                "    title=\"Sales Channel\",                                                       # Executing this step in data analysis\n",
                "    bbox_to_anchor=(1, 1),                                                       # Executing this step in data analysis\n",
                "    loc='upper right',                                                           # Executing this step in data analysis\n",
                "    frameon=True,                                                                # Executing this step in data analysis\n",
                "    framealpha=1,                                                                # Setting the transparency of the dots\n",
                "    edgecolor='black',                                                           # Executing this step in data analysis\n",
                "    facecolor='white'                                                            # Executing this step in data analysis\n",
                ")                                                                                # Executing this step in data analysis\n",
                "\n",
                "plt.tight_layout()                                                               # Adjusting the spacing between subplots\n",
                "plt.show()                                                                       # Displaying the final plots on the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Batuhan - Research Question\n",
                "**Impact of Unit Price and Discount Rate on Sales Volume across Channels**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv('cleaned_winter_research_data.csv')                             # Reading the data from a CSV file\n",
                "\n",
                "df_plot = df.dropna(subset=['unit_price_usd', 'units_sold', 'sales_channel', 'discount_rate']) # Executing this step in data analysis\n",
                "\n",
                "plt.figure(figsize=(12, 8))                                                      # Using different visuals to show data groups\n",
                "\n",
                "plot = sns.scatterplot(                                                          # Drawing a scatter plot with dots\n",
                "    data=df_plot,                                                                # Executing this step in data analysis\n",
                "    x='unit_price_usd',                                                          # Executing this step in data analysis\n",
                "    y='units_sold',                                                              # Executing this step in data analysis\n",
                "    hue='sales_channel',                                                         # Using different visuals to show data groups\n",
                "    size='discount_rate',                                                        # Using different visuals to show data groups\n",
                "    sizes=(20, 200),                                                             # Executing this step in data analysis\n",
                "    alpha=0.7,                                                                   # Setting the transparency of the dots\n",
                "    palette='deep'                                                               # Choosing the colors to be used in plot\n",
                ")                                                                                # Executing this step in data analysis\n",
                "\n",
                "plt.title('Impact of Unit Price and Discount Rate on Sales Volume across Channels', fontsize=15) # Using different visuals to show data groups\n",
                "plt.xlabel('Unit Price (USD)', fontsize=12)                                      # Using different visuals to show data groups\n",
                "plt.ylabel('Units Sold', fontsize=12)                                            # Using different visuals to show data groups\n",
                "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Channels & Discounts') # Showing the guide box for colors and symbols\n",
                "plt.grid(True, linestyle='--', alpha=0.6)                                        # Showing grid lines in the background\n",
                "\n",
                "plt.tight_layout()                                                               # Adjusting the spacing between subplots\n",
                "plt.show()                                                                       # Displaying the final plots on the screen\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Oğuz Kerem Ayhan - Research Question\n",
                "**How do Price Sensitivity (unitprice_usd) and Sugar Content (sugar_g) impact Units Sold across different Diet Labels?**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Load the cleaned dataset from the CSV file\n",
                "df = pd.read_csv('cleaned_winter_research_data.csv')                             # Reading the data from a CSV file\n",
                "\n",
                "# Set the seaborn theme to 'white' and palette to 'muted' for academic publication quality\n",
                "sns.set_theme(style=\"white\", palette=\"muted\")                                    # Setting the visual style for the plots\n",
                "# Update font sizes for better readability in the report\n",
                "plt.rcParams.update({'font.size': 10, 'axes.labelsize': 12, 'axes.titlesize': 14}) # Changing the font sizes for better display\n",
                "\n",
                "# Initialize a figure with 2 rows and 1 column for the subplots, setting the size\n",
                "fig, axes = plt.subplots(2, 1, figsize=(12, 14))                                 # Creating a figure with multiple empty plots\n",
                "\n",
                "# Plot 1: Scatter plot with regression lines showing the relationship between Unit Price and Units Sold\n",
                "# This visualization helps understand price elasticity: How unit price affects sales volume across diff diet labels\n",
                "sns.scatterplot(ax=axes[0], data=df, x='unit_price_usd', y='units_sold',         # Drawing a scatter plot with dots\n",
                "                hue='diet_label', alpha=0.5, s=60)                               # Using different visuals to show data groups\n",
                "\n",
                "# Add a regression line for 'regular' diet label products (Blue)\n",
                "sns.regplot(ax=axes[0], data=df[df['diet_label'] == 'regular'], x='unit_price_usd', y='units_sold',  # Adding a trend line to the scatter plot\n",
                "            scatter=False, color='blue', label='Regular Trend')                  # Executing this step in data analysis\n",
                "\n",
                "# Add a regression line for 'lowsugar' diet label products (Green)\n",
                "sns.regplot(ax=axes[0], data=df[df['diet_label'] == 'lowsugar'], x='unit_price_usd', y='units_sold',  # Adding a trend line to the scatter plot\n",
                "            scatter=False, color='green', label='Low-Sugar Trend')               # Executing this step in data analysis\n",
                "\n",
                "# Set title and labels for the first plot\n",
                "axes[0].set_title('Impact of Unit Price on Sales by Diet Label')                 # Giving a title to this specific plot\n",
                "axes[0].set_xlabel('Unit Price (USD)')                                           # Adding a label to the horizontal x-axis\n",
                "axes[0].set_ylabel('Units Sold')                                                 # Adding a label to the vertical y-axis\n",
                "# Add a grid for easier reading of values\n",
                "axes[0].grid(True, linestyle='--', alpha=0.6)                                    # Showing grid lines in the background\n",
                "# Add a legend to the upper left, outside the plot area\n",
                "axes[0].legend(title='Diet Label', bbox_to_anchor=(1.05, 1), loc='upper left')   # Showing the guide box for colors and symbols\n",
                "\n",
                "# Plot 2: Scatter plot with regression lines showing the relationship between Sugar Content and Units Sold\n",
                "# This visualization investigates how sugar content classification influences consumer demand\n",
                "sns.scatterplot(ax=axes[1], data=df, x='sugar_g', y='units_sold',                # Drawing a scatter plot with dots\n",
                "                hue='diet_label', palette='viridis', alpha=0.5, s=60)            # Using different visuals to show data groups\n",
                "\n",
                "# Add an overall regression trend line for Sugar Content vs Sales (Black dashed line)\n",
                "sns.regplot(ax=axes[1], data=df, x='sugar_g', y='units_sold', scatter=False, color='black',  # Adding a trend line to the scatter plot\n",
                "            line_kws={\"linestyle\": \"--\"}, label='Overall Trend')                 # Executing this step in data analysis\n",
                "\n",
                "# Set title and labels for the second plot\n",
                "axes[1].set_title('Impact of Sugar Content on Sales Across Diet Categories')     # Giving a title to this specific plot\n",
                "axes[1].set_xlabel('Sugar (grams)')                                              # Adding a label to the horizontal x-axis\n",
                "axes[1].set_ylabel('Units Sold')                                                 # Adding a label to the vertical y-axis\n",
                "# Add a grid for easier reading of values\n",
                "axes[1].grid(True, linestyle='--', alpha=0.6)                                    # Showing grid lines in the background\n",
                "# Add a legend to the upper left\n",
                "axes[1].legend(title='Diet Label', bbox_to_anchor=(1.05, 1), loc='upper left')   # Showing the guide box for colors and symbols\n",
                "\n",
                "# Adjust layout to prevent overlap between subplots\n",
                "plt.tight_layout()                                                               # Adjusting the spacing between subplots\n",
                "# Display the final figure\n",
                "plt.show()                                                                       # Displaying the final plots on the screen\n",
                "\n",
                "# Statistical Summary: Calculate and print the Pearson Correlation Matrix to quantify relationships\n",
                "correlation_matrix = df[['unit_price_usd', 'sugar_g', 'units_sold']].corr()      # Calculating the correlation numbers between variables\n",
                "print(\"Pearson Correlation Matrix:\\n\", correlation_matrix)                       # Printing information to the screen\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
